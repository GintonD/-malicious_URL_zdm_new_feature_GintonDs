{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0d07650068d61125eebdc0d5f99de5106bd98522aef3474d27d944b0e7503d5e9",
   "display_name": "Python 3.8.3rc1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d07650068d61125eebdc0d5f99de5106bd98522aef3474d27d944b0e7503d5e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from ELM import ELMClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_check = {\n",
    "\t\"base\": {\n",
    "\t\t\"features\"     : [1,2,3,4,5,6,7,8,9],\n",
    "\t\t\"C\"            : 0.001,\n",
    "\t\t\"n_hidden\"     : 50,\n",
    "\t\t\"y_column_idx\" : 10,\n",
    "\t\t\"feature_file\" : \"../Datasets/features_extractions/base_(all).csv\"\n",
    "\t},\n",
    "\t\"base_robust\": {\n",
    "\t\t\"features\"     : [2,6,8,9],\n",
    "\t\t\"C\"            : 0.001,\n",
    "\t\t\"n_hidden\"     : 10,\n",
    "\t\t\"y_column_idx\" : 10,\n",
    "\t\t\"feature_file\" : \"../Datasets/features_extractions/base_(all).csv\"\n",
    "\t},\n",
    "\t\"all\": {\n",
    "\t\t\"features\"     : [1,2,3,4,5,6,7,8,9,10,11,13,15],\n",
    "\t\t\"C\"            : 50,\n",
    "\t\t\"n_hidden\"     : 150,\n",
    "\t\t\"y_column_idx\" : 17,\n",
    "\t\t\"feature_file\" : \"../Datasets/features_extractions/median_9_2_(25-75)_vt_include.csv\"\n",
    "\t},\n",
    "\t\"novel\": {\n",
    "\t\t\"features\"     : [10,11,13,15],\n",
    "\t\t\"C\"            : 0.004,\n",
    "\t\t\"n_hidden\"     : 50,\n",
    "\t\t\"y_column_idx\" : 17,\n",
    "\t\t\"feature_file\" : \"../Datasets/features_extractions/median_9_2_(25-75)_vt_include.csv\"\n",
    "\t},\n",
    "\t\"hybrid_robust\": {\n",
    "\t\t\"features\"     : [2,6,8,9,10,11,13,15],\n",
    "\t\t\"C\"            : 0.01,\n",
    "\t\t\"n_hidden\"     : 100,\n",
    "\t\t\"y_column_idx\" : 17,\n",
    "\t\t\"feature_file\" : \"../Datasets/features_extractions/median_9_2_(25-75)_vt_include.csv\"\n",
    "\t}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-215220cc5765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_size\u001b[0m               \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpath\u001b[0m               \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mfeatures_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../Datasets/features_extractions/median_9_2_(75-25)_vt_include.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfeatures_file\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "features_to_check = [\"base\",\"base_robust\",\"all\",\"novel\",\"hybrid_robust\"]\n",
    "\n",
    "threshold       = 0.5\n",
    "learning_rate   = 0.001\n",
    "n_splits\t\t= 10\n",
    "test_size\t\t= 0.25\n",
    "\n",
    "path               = os.path.dirname(os.path.abspath(__file__))\n",
    "features_file_name = \"../Datasets/features_extractions/median_9_2_(75-25)_vt_include.csv\"\n",
    "features_file      = os.path.join(path, features_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features_set in features_to_check:\n",
    "\tprint(\"\\n\\nChecking features - %s\" % (features_set))\n",
    "\tfeatures_file = os.path.join(path, features_check[features_set][\"feature_file\"])\n",
    "\ty_column_idx  = features_check[features_set][\"y_column_idx\"]\n",
    "\tn_hidden      = features_check[features_set][\"n_hidden\"]\n",
    "\ttrain         = pd.read_csv(features_file)\n",
    "\t######## Append artificial data by number of consecutive characters feature ########\n",
    "\tif 2 in features_check[features_set][\"features\"]:\n",
    "\t\tmal         = train[train[train.columns[y_column_idx]]==1].sample(500).copy()\n",
    "\t\tmal[\"2\"]    = mal[\"2\"].apply(lambda x:x*random.randint(3,9))\n",
    "\t\ttrain = train.append(mal, ignore_index=True)\n",
    "\t######################################## END #######################################\n",
    "\tuse_columns   = features_check[features_set][\"features\"]\n",
    "\tuse_columns.append(y_column_idx)\n",
    "\n",
    "\ttrain = train[train.columns[use_columns]]\n",
    "\n",
    "\tuse_dataset       = train.copy()\n",
    "\tuse_dataset       = np.asfarray(use_dataset.values,np.dtype('Float64'))\n",
    "\t# Normlize the dataset\n",
    "\tscaler = MinMaxScaler().fit(use_dataset[:, :-1])\n",
    "\tdataset_norm = scaler.transform(use_dataset[:, :-1])\n",
    "\n",
    "\t# Split features and labels\n",
    "\tX, y = use_dataset, np.transpose([use_dataset[:, -1]])\n",
    "\n",
    "\tindices = np.arange(y.shape[0])\n",
    "\tX_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, indices, stratify=y, test_size=test_size, random_state=42)\n",
    "kf = KFold(n_splits=n_splits, random_state=None, shuffle=False)\n",
    "kf.get_n_splits(X_train)\n",
    "for train_index, test_index in kf.split(idx_train):\n",
    "\t\tX_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "\t\ty_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../ann_model_base_t_scaler.sav\"\n",
    "ann_model=pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=\"../enn_model.sav\"\n",
    "elm_model=pickle.load(open(fname,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[ann_model,elm_model]\n",
    "stacking = StackingClassifier(estimators=models, final_estimator=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "score() missing 2 required positional arguments: 'X' and 'y'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-929802a39080>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstacking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: score() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "stacking.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}